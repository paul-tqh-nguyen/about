<div class="row">
  <div class="portfolio-item">
    <h3>Impact of Attention</h3>
    <h4>A demonstration of how attention mechanisms can improve the accuracy of RNN architectures</h4>
    <br>
    <table style="width:100%">
      <tr>
        <th>
	  <center>
	    <img src="./img/impact_of_attention/architecture.png" alt="" style="width:60vw;" hspace="5">
	  </center>
	</th>
      </tr>
    </table>
    <br>
    <p>This work gives empircal evidence of how attention mechanisms (in particular, <a href="https://arxiv.org/abs/1703.03130">Zhouhan Lin's self-attention mechanism</a>) can improve the accuracy of an RNN-style NLP architecture (in particular, LSTM architectures) on text classification.</p>
    <p>Source Code: <a href="https://github.com/paul-tqh-nguyen/impact_of_attention">GitHub</a></p>
    <p>Results: <a href="https://paul-tqh-nguyen.github.io/impact_of_attention/">Result Summary</a></p>
  </div>
</div>
